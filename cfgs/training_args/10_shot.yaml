# args for Trainer
# see https://huggingface.co/docs/transformers/v4.47.1/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments
output_dir: ./ckpt/{aug_method}/{dataset}_{k_shot}-shot_{seed}
do_train: true
do_eval: true
eval_strategy: "epoch"
per_device_train_batch_size: 24
per_device_eval_batch_size: 24
gradient_accumulation_steps: 1
learning_rate: 5e-5
weight_decay: 0.01
lr_scheduler_type: "constant"
num_train_epochs: 100
logging_strategy: 'epoch'
#logging_steps: 100
save_strategy: "epoch"
save_total_limit: 1
save_only_model: true
seed: 22
fp16: true
#fp16_full_eval: true
dataloader_num_workers: 4
load_best_model_at_end: true
metric_for_best_model: f1
greater_is_better: true
report_to: wandb
run_name: "{aug_method}_{dataset}_{k_shot}_{seed}"
torch_compile: true
predict_with_generate: true
generation_max_length: 64
generation_num_beams: 4